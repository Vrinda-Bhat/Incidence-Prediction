{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Test, master=local[*]) created by __init__ at <ipython-input-1-8d78359fe3e3>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-504-8d78359fe3e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Test, master=local[*]) created by __init__ at <ipython-input-1-8d78359fe3e3>:4 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Test\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\vrinda.b\\Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_incdf_path = str(path+r\"\\final_all_incdf_test - Copy.csv\")\n",
    "all_chandf_path = path+r\"\\all_chandf_test.csv\"\n",
    "all_problemdf_path = path+r\"\\all_problemdf_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_incdf = sc.textFile(all_incdf_path)\n",
    "all_chandf = sc.textFile(all_chandf_path)\n",
    "all_problemdf = sc.textFile(all_problemdf_path)\n",
    "all_outagedf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_incdf.collect()\n",
    "#all_chandf.collect()\n",
    "#all_problemdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_incdf_df = sqlContext.createDataFrame(all_incdf, ['sys_id','app_name','ci_class','cmdb_ci','sys_class_name','opened_at','major_incident','minor_incident','incident_monitoring','caused_by','contact_type','incident_category'])\n",
    "#all_incdf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "all_incdf_df = sqlContext.read.csv(all_incdf_path,header = True)\n",
    "all_incdf_df = all_incdf_df.withColumn(\"Date\", func.to_date(\"opened_at\", \"dd-MM-yyyy\"))\n",
    "#all_incdf_df.show()\n",
    "#all_incdf_df.select('Date').show()\n",
    "\n",
    "#Dataframe Creation\n",
    "all_chandf_df = sqlContext.read.csv(all_chandf_path,header = True)\n",
    "all_chandf_df = all_chandf_df.withColumn(\"Date\", func.to_date(\"start_date\", \"dd-MM-yyyy\"))\n",
    "all_problemdf_df = sqlContext.read.csv(all_problemdf_path,header = True)\n",
    "all_probelmdf_df = all_problemdf_df.withColumn(\"Date\", func.to_date(\"opened_at\", \"dd-MM-yyyy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_window_lag_size = 7\n",
    "prediction_window_size = 4\n",
    "major_incident_column_name = 'major_incident'\n",
    "target_column_name = major_incident_column_name+ str('_count')\n",
    "custom_date_column_name = 'custom_date'\n",
    "ci_name_column = 'app_name'\n",
    "\n",
    "incident_date_column = 'opened_at'\n",
    "change_date_column = 'start_date'\n",
    "problem_date_column = 'opened_at'\n",
    "outage_date_column = 'sys_created_on'\n",
    "\n",
    "cis_list = ['xStore_invalidCI',\n",
    " 'Oracle Retail Xstore Point-of-Service (POS)',\n",
    " 'Triversity',\n",
    " 'Point of Sale (POS) Register â€“ TJX',\n",
    " 'Windows',\n",
    " 'Oracle Retail Xstore Office (Xadmin)',\n",
    " 'Sortation Servers; Sorter Hardware',\n",
    " 'System Control & Automation Network (SCAN) AS400',\n",
    " 'Network Intelligence',\n",
    " 'ServiceNow'\n",
    "]   \n",
    "ci_name = cis_list[0]\n",
    "category = None\n",
    "\n",
    "final_df = None\n",
    "\n",
    "model_training = True\n",
    "ci_name = cis_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "def to_date_converter(input_dataset, ts_column_name):\n",
    "    input_dataset = input_dataset.withColumn(\"Date\", func.to_date(ts_column_name, \"dd-MM-yyyy\"))\n",
    "    return input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_aggregation(input_dataset, agg_column_name):\n",
    "    input_dataset = to_date_converter(input_dataset, agg_column_name)\n",
    "    #(c) for c in df.columns\n",
    "    #agg_column_list = input_dataset.select(c for c in df.columns).map(lambda r :r.getString(0)).collect.toList \n",
    "    agg_column_list = input_dataset.columns\n",
    "    agg_column_list.remove(agg_column_name)\n",
    "    aggregated_dataset = date_aggregator(input_dataset, agg_column_name, agg_column_list)\n",
    "    return aggregated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(left_dataset, right_dataset, left_dataset_column_name, right_dataset_column_name, join_type='outer')\n",
    "    merged_dataset = left_dataset.join(right_dataset,(left_dataset.left_dataset_column_name==right_dataset.right_dataset_column_name),joinType=\"outer\")\n",
    "    merged_dataset[merged_column_name] = merged_dataset[[left_dataset_column_name,right_dataset_column_name]].apply(lambda row:row[0] if pd.notnull(row[0]) else row[1], axis=1)\n",
    "    merged_dataset_columns = left_dataset.columns + right_dataset.columns\n",
    "    merged_dataset_columns.remove(left_dataset_column_name)\n",
    "    merged_dataset_columns.remove(right_dataset_column_name)\n",
    "    merged_dataset_columns.append(merged_column_name)\n",
    "    merged_dataset = merged_dataset[merged_dataset_columns]\n",
    "    merged_dataset.withColumnRenamed(columns = {merged_column_name:custom_date_column_name}, inplace =True)\n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_merging_dataset(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column):\n",
    "    JOIN = 'outer'\n",
    "    incdf_gp = dataset_aggregation(all_incdf_df, incident_date_column)\n",
    "    chandf_gp = dataset_aggregation(all_chandf_df, change_date_column)\n",
    "    problemdf_gp = None\n",
    "    outagedf_gp = None\n",
    "    if(all_problemdf_df is not None):\n",
    "        problemdf_gp = dataset_aggregation(all_problemdf_df, problem_date_column)\n",
    "    if(all_outagedf_df is not None):\n",
    "        outagedf_gp = dataset_aggregation(all_outagedf_df, outage_date_column)\n",
    "\n",
    "    merged_dataset = merge_datasets(chandf_gp, incdf_gp, change_date_column, incident_date_column, JOIN)\n",
    "    if(problemdf_gp is not None):\n",
    "        merged_dataset = merge_datasets(merged_dataset, problemdf_gp, custom_date_column_name, problem_date_column, JOIN)\n",
    "    if(outagedf_gp is not None):\n",
    "        merged_dataset = merge_datasets(merged_dataset, outagedf_gp, custom_date_column_name, outage_date_column, JOIN)\n",
    "    \n",
    "    merged_dataset = merged_dataset.fillna(0)\n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "def window_values_aggregation_next(row, column_name, target_column_name, history_window_lag_size, prediction_window_size, is_training):\n",
    "    sum_value = 0.0\n",
    "    std_value = 0.0\n",
    "    min_value = 0.0\n",
    "    max_value = 0.0\n",
    "    avg_slope = 0.0\n",
    "    avg_slope_sign = 0.0\n",
    "    result_array = np.array([])\n",
    "    #result_arr =spark.createDataFrame(sc.emptyRDD(), schema)\n",
    "    \n",
    "    if(is_training):\n",
    "        window_size = history_window_lag_size+prediction_window_size\n",
    "    else:\n",
    "        window_size = history_window_lag_size\n",
    "    \n",
    "    for i in range(0, window_size):\n",
    "        \n",
    "        if (column_name != target_column_name):\n",
    "            if (i >= prediction_window_size) & (i < window_size):\n",
    "                dataset_coulmn_name = (column_name+'_'+str(i))\n",
    "                result_array = np.append(result_array, row[dataset_coulmn_name])\n",
    "                #print(\"dataset_coulmn_name : \",dataset_coulmn_name )\n",
    "                #print('Predictors loop over i: ', i)\n",
    "            \n",
    "        elif (column_name == target_column_name) & (is_training):\n",
    "            if (i < prediction_window_size):\n",
    "                dataset_coulmn_name = (column_name+'_'+str(i))\n",
    "                result_array = np.append(result_array, row[dataset_coulmn_name])\n",
    "                #print(\"dataset_coulmn_name : \",dataset_coulmn_name )\n",
    "                #print('Target loop over i: ', i)\n",
    "    \n",
    "    if column_name != target_column_name: \n",
    "        sum_value = np.sum(result_array)\n",
    "        std_value = np.std(result_array)\n",
    "        min_value = np.amin(result_array)\n",
    "        max_value = np.amax(result_array)\n",
    "        \n",
    "        start_val = result_array[0]\n",
    "        for i in range(1, len(result_array)):\n",
    "            avg_slope = avg_slope + ((result_array[i]-start_val)/i)\n",
    "        if avg_slope > 0.0 :\n",
    "            avg_slope_sign = 1.0\n",
    "        elif avg_slope < 0.0 :\n",
    "            avg_slope_sign = -1.0\n",
    "        \n",
    "        return pd.Series([sum_value, std_value, min_value, max_value, avg_slope, avg_slope_sign])\n",
    "    \n",
    "    elif (column_name == target_column_name) & (is_training):\n",
    "        sum_value = np.sum(result_array)\n",
    "        return pd.Series([sum_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_aggregator(input_dataset, date_column_name, agg_column_list):\n",
    "    agg_dict = {}\n",
    "    new_column_dict = {}\n",
    "    for column_name in agg_column_list:\n",
    "        agg_dict[column_name] = \"sum\"\n",
    "        new_column_dict[column_name] = str(column_name+\"_count\")\n",
    "        \n",
    "    agg_dataset = input_dataset.groupby([date_column_name]).agg(agg_dict).withColumnRenamed(columns=new_column_dict).reset_index(drop=False, inplace=True)\n",
    "    return agg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def history_window_lag_dataset(_dataset, date_column_name, history_window_lag_size):\n",
    "    #input_dataset = _dataset.sort_values(by=date_column_name, ascending=True)\n",
    "    input_dataset =_dataset.orderBy(date_column_name,ascending = True)\n",
    "    #input_dataset_columns = input_dataset.select()\n",
    "    input_dataset_columns = input_dataset.columns\n",
    "    shift_dataset=input_dataset.select([date_column_name])\n",
    "    for i in range(0, history_window_lag_size):\n",
    "        for col in input_dataset_columns:\n",
    "            w = Window().partitionBy().orderBy(col)\n",
    "            if col != date_column_name:\n",
    "                x = input_dataset.select(\"*\", lag(col,1).over(w).alias(col+'_'+str(i)))\n",
    "                shift_dataset = shift_dataset.join(x)\n",
    "                #.shift(i).alias(col+'_'+str(i)), axis=1)\n",
    "        \n",
    "    return shift_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applying_filter_on_datasets(\n",
    "        all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, \n",
    "        change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column):\n",
    "\n",
    "  \n",
    "    incdf = all_incdf_df.filter(col(ci_name_column).isin(\"ci_name\") == True)#.rdd.flatMap(lambda x: x).collect()\n",
    "    incdf = incdf.select([incident_date_column, \"major_incident\", \"minor_incident\", \"incident_monitoring\"])\n",
    "    incdf = incdf.dropna()\n",
    "    \n",
    "    chandf = all_chandf_df.filter(col(ci_name_column).isin(\"ci_name\") == True)#.rdd.flatMap(lambda x: x).collect()\n",
    "    chandf = chandf.dropna()\n",
    "    chandf = chandf.withColumn(\"change\", lit(1))\n",
    "    chandf = chandf.select([change_date_column, \"change\"])\n",
    "        \n",
    "\n",
    "    \n",
    "    problemdf = None\n",
    "    if(all_problemdf is not None):\n",
    "        problemdf = all_problemdf_df.filter(col(ci_name_column).isin(\"ci_name\") == True)#.rdd.flatMap(lambda x: x).collect()\n",
    "        problemdf = problemdf.dropna()\n",
    "        problemdf = problemdf.withColumn(\"problem\", lit(1))\n",
    "        problemdf = problemdf.select([problem_date_column, \"problem\"])\n",
    "    \n",
    "    outagedf = None\n",
    "    if(all_outagedf is not None):\n",
    "        outagedf = all_outagedf_df.filter(col(ci_name_column).isin(\"ci_name\") == True)#.rdd.flatMap(lambda x: x).collect()\n",
    "        outagedf = outagedf.dropna()\n",
    "        outagedf = outagedf.withColumn(\"outage\", lit(1))\n",
    "        outagedf = outagedf.select([outage_date_column, \"outage\"])\n",
    "\n",
    "    return (incdf, chandf, problemdf, outagedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, \n",
    "        change_date_column, problem_date_column, outage_date_column, history_window_lag_size, \n",
    "                        prediction_window_size, custom_date_column_name, target_column_name, is_training):\n",
    "\n",
    "    merged_dataset = process_merging_dataset(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column)\n",
    "    \n",
    "    if(is_training):\n",
    "        window_size = history_window_lag_size+prediction_window_size\n",
    "    else:\n",
    "        window_size = history_window_lag_size\n",
    "        \n",
    "    lagged_dataset = history_window_lag_dataset(merged_dataset, custom_date_column_name, window_size)\n",
    "    \n",
    "    merged_dataset_columns = merged_dataset.columns\n",
    "    shift_dataset=lagged_dataset[[custom_date_column_name]]\n",
    "    #print(merged_dataset.head)\n",
    "    #print(lagged_dataset.head(100))\n",
    "    for col in merged_dataset_columns:\n",
    "        if col != custom_date_column_name :\n",
    "            if col != target_column_name:\n",
    "                shift_dataset[[col, col.split('_')[0]+\"_std\", col.split('_')[0]+\"_min\", col.split('_')[0]+\"_max\", col.split('_')[0]+\"_avg_slope\", col.split('_')[0]+\"_avg_slope_dirn\"]] = lagged_dataset.apply(window_values_aggregation_next, args=(col, target_column_name, history_window_lag_size, prediction_window_size, is_training), axis=1)\n",
    "            elif (col == target_column_name) & (is_training) :\n",
    "                shift_dataset[col] = lagged_dataset.apply(window_values_aggregation_next, args=(col, target_column_name, history_window_lag_size, prediction_window_size, is_training), axis=1)\n",
    "                print('Shifted data : ',target_column_name, shift_dataset[[target_column_name]].groupby([target_column_name]).count())  \n",
    "                    \n",
    "    print(merged_dataset.shape, shift_dataset.shape, shift_dataset.dropna().shape)\n",
    "    shift_dataset = shift_dataset.dropna()\n",
    "\n",
    "    return shift_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "def application_processing(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name, is_training):\n",
    "\n",
    "    outagedf = None\n",
    "    incdf, chandf, problemdf, outagedf = applying_filter_on_datasets(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf, incident_date_column, change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column)\n",
    "\n",
    "    prepared_dataset = dataset_preparation(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df,incident_date_column, change_date_column, problem_date_column, outage_date_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name, is_training)\n",
    "    \n",
    "    prepared_dataset.app_name = ci_name\n",
    "    \n",
    "    if(is_training):\n",
    "        prepared_dataset =prepared_dataset.withColumn('p1_flag', when(col('target_column_name') == 0,0,1))\n",
    "    \n",
    "    return prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'sum(`Date`)' due to data type mismatch: function sum requires numeric types, not date;;\\n'Aggregate [opened_at#4093], [opened_at#4093, sum(cast(caused_by#4097 as double)) AS sum(caused_by)#4275, sum(cast(sys_id#4088 as double)) AS sum(sys_id)#4276, sum(cast(contact_type#4098 as double)) AS sum(contact_type)#4277, sum(cast(major_incident#4094 as double)) AS sum(major_incident)#4278, sum(cast(cmdb_ci#4091 as double)) AS sum(cmdb_ci)#4279, sum(cast(incident_monitoring#4096 as double)) AS sum(incident_monitoring)#4280, sum(cast(incident_category#4099 as double)) AS sum(incident_category)#4281, sum(cast(app_name#4089 as double)) AS sum(app_name)#4282, sum(cast(ci_class#4090 as double)) AS sum(ci_class)#4283, sum(Date#4248) AS sum(Date)#4284, sum(cast(minor_incident#4095 as double)) AS sum(minor_incident)#4285, sum(cast(sys_class_name#4092 as double)) AS sum(sys_class_name)#4286]\\n+- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4248]\\n   +- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4112]\\n      +- Relation[sys_id#4088,app_name#4089,ci_class#4090,cmdb_ci#4091,sys_class_name#4092,opened_at#4093,major_incident#4094,minor_incident#4095,incident_monitoring#4096,caused_by#4097,contact_type#4098,incident_category#4099] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3170.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'sum(`Date`)' due to data type mismatch: function sum requires numeric types, not date;;\n'Aggregate [opened_at#4093], [opened_at#4093, sum(cast(caused_by#4097 as double)) AS sum(caused_by)#4275, sum(cast(sys_id#4088 as double)) AS sum(sys_id)#4276, sum(cast(contact_type#4098 as double)) AS sum(contact_type)#4277, sum(cast(major_incident#4094 as double)) AS sum(major_incident)#4278, sum(cast(cmdb_ci#4091 as double)) AS sum(cmdb_ci)#4279, sum(cast(incident_monitoring#4096 as double)) AS sum(incident_monitoring)#4280, sum(cast(incident_category#4099 as double)) AS sum(incident_category)#4281, sum(cast(app_name#4089 as double)) AS sum(app_name)#4282, sum(cast(ci_class#4090 as double)) AS sum(ci_class)#4283, sum(Date#4248) AS sum(Date)#4284, sum(cast(minor_incident#4095 as double)) AS sum(minor_incident)#4285, sum(cast(sys_class_name#4092 as double)) AS sum(sys_class_name)#4286]\n+- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4248]\n   +- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4112]\n      +- Relation[sys_id#4088,app_name#4089,ci_class#4090,cmdb_ci#4091,sys_class_name#4092,opened_at#4093,major_incident#4094,minor_incident#4095,incident_monitoring#4096,caused_by#4097,contact_type#4098,incident_category#4099] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:169)\r\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:188)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-523-c9abad81e89a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplication_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_problemdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_outagedf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincident_date_column\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchange_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutage_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci_name_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_window_lag_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_window_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_date_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_column_name\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mis_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final dataframe shape : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-522-5aee5c1635ba>\u001b[0m in \u001b[0;36mapplication_processing\u001b[1;34m(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name, is_training)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mincdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchandf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblemdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutagedf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplying_filter_on_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_problemdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_outagedf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincident_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutage_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci_name_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprepared_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_preparation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_problemdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_outagedf_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mincident_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutage_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_window_lag_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_window_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_date_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprepared_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mci_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-521-65be255f242a>\u001b[0m in \u001b[0;36mdataset_preparation\u001b[1;34m(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name, is_training)\u001b[0m\n\u001b[0;32m      3\u001b[0m                         prediction_window_size, custom_date_column_name, target_column_name, is_training):\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmerged_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_merging_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_problemdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_outagedf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincident_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutage_date_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-516-9a772a6115c0>\u001b[0m in \u001b[0;36mprocess_merging_dataset\u001b[1;34m(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf_df, incident_date_column, change_date_column, problem_date_column, outage_date_column)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_merging_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_problemdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_outagedf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincident_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproblem_date_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutage_date_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mJOIN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'outer'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mincdf_gp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_aggregation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_incdf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincident_date_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mchandf_gp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_aggregation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_chandf_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchange_date_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mproblemdf_gp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-514-08dbda605c5b>\u001b[0m in \u001b[0;36mdataset_aggregation\u001b[1;34m(input_dataset, agg_column_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0magg_column_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0magg_column_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_column_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maggregated_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_aggregator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_column_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maggregated_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-518-6f3aa1ffe7da>\u001b[0m in \u001b[0;36mdate_aggregator\u001b[1;34m(input_dataset, date_column_name, agg_column_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mnew_column_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0magg_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_column_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_column_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0magg_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exprs should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# Columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve 'sum(`Date`)' due to data type mismatch: function sum requires numeric types, not date;;\\n'Aggregate [opened_at#4093], [opened_at#4093, sum(cast(caused_by#4097 as double)) AS sum(caused_by)#4275, sum(cast(sys_id#4088 as double)) AS sum(sys_id)#4276, sum(cast(contact_type#4098 as double)) AS sum(contact_type)#4277, sum(cast(major_incident#4094 as double)) AS sum(major_incident)#4278, sum(cast(cmdb_ci#4091 as double)) AS sum(cmdb_ci)#4279, sum(cast(incident_monitoring#4096 as double)) AS sum(incident_monitoring)#4280, sum(cast(incident_category#4099 as double)) AS sum(incident_category)#4281, sum(cast(app_name#4089 as double)) AS sum(app_name)#4282, sum(cast(ci_class#4090 as double)) AS sum(ci_class)#4283, sum(Date#4248) AS sum(Date)#4284, sum(cast(minor_incident#4095 as double)) AS sum(minor_incident)#4285, sum(cast(sys_class_name#4092 as double)) AS sum(sys_class_name)#4286]\\n+- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4248]\\n   +- Project [sys_id#4088, app_name#4089, ci_class#4090, cmdb_ci#4091, sys_class_name#4092, opened_at#4093, major_incident#4094, minor_incident#4095, incident_monitoring#4096, caused_by#4097, contact_type#4098, incident_category#4099, to_date('opened_at, Some(dd-MM-yyyy)) AS Date#4112]\\n      +- Relation[sys_id#4088,app_name#4089,ci_class#4090,cmdb_ci#4091,sys_class_name#4092,opened_at#4093,major_incident#4094,minor_incident#4095,incident_monitoring#4096,caused_by#4097,contact_type#4098,incident_category#4099] csv\\n\""
     ]
    }
   ],
   "source": [
    "final_df = application_processing(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf, incident_date_column,change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name,  is_training = model_training)\n",
    "\n",
    "print(\"Final dataframe shape : \", final_df.shape)\n",
    "print()\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print()\n",
    "for i in range(1,len(cis_list)):\n",
    "    ci_name = cis_list[i]\n",
    "    new_app_df = application_processing(all_incdf_df, all_chandf_df, all_problemdf_df, all_outagedf, incident_date_column, change_date_column, problem_date_column, outage_date_column, ci_name, category, ci_name_column, history_window_lag_size, prediction_window_size, custom_date_column_name, target_column_name,  is_training = model_training)\n",
    "    # ========================= Concate App DF ====================================\n",
    "    final_df = final_df.join(new_app_df)\n",
    "    print()\n",
    "    print(\"Final dataframe shape : \", final_df.shape)\n",
    "    print()\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
